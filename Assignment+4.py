
# coding: utf-8

# ---
# 
# _You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._
# 
# ---

# # Assignment 4

# In[2]:


import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
# 
# ## Part 1 - Random Graph Identification
# 
# For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

# In[2]:


P1_Graphs = pickle.load(open('A4_graphs','rb'))


# In[ ]:





# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)
# 
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
# 
# *The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

# In[3]:


def graph_identification():
    
#     def plot_power_law(G):
#         degrees = G.degree()
#         degree_values = sorted(set(degrees.values()))
#         histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]


#     def print_average_shortets_paths(list_of_Gs):
#         shortest_paths = []
#         clustering_coeficients = []
#         for G in list_of_Gs:
#             #look for small averge shortest path for low values of p
#             shortest_paths.append(nx.average_shortest_path_length(G))
#             #look for high clustering coeficient for small world both low p
#             clustering_coeficients.append(nx.average_clustering(G))
#         summary = pd.DataFrame()  
#         summary['graph'] = list(range(1,6))
#         summary["AVG shortest Path"] = shortest_paths
#         summary['clustering coe'] = clustering_coeficients
#         return summary
    
#     G1 = P1_Graphs[0]
#     G2 = P1_Graphs[1]
#     G3 = P1_Graphs[2]
#     G4 = P1_Graphs[3]
#     G5 = P1_Graphs[4]
    
    
    # Your Code Here
    
    G1 = 'PA'
    G2 = 'SW_L'
    G3 = 'SW_L'
    G4 = 'PA'
    G5 =  'SW_H'
    return list([G1,G2,G3,G4,G5]) # Your Answer Here


# ---
# 
# ## Part 2 - Company Emails
# 
# For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
# 
# The network also contains the node attributes `Department` and `ManagementSalary`.
# 
# `Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary.

# In[6]:


G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))


# ### Part 2A - Salary Prediction
# 
# Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.
# 
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.
# 
#     Example:
#     
#         1       1.0
#         2       0.0
#         5       0.8
#         8       1.0
#             ...
#         996     0.7
#         1000    0.5
#         1001    0.0
#         Length: 252, dtype: float64

# In[5]:


def salary_predictions():
    
    # Your Code Here
    df = pd.DataFrame(index=G.nodes())
    df['management'] = pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))
    #df['department'] = pd.Series(nx.get_node_attributes(G, 'Department'))
    df['degree'] = pd.Series(G.degree())
    df['clustering_coe'] = pd.Series(nx.clustering(G))
    df['degree_centrality'] = pd.Series(nx.degree_centrality(G)) 
    df['closeness_centrality'] = pd.Series(nx.closeness_centrality(G)) 
    df['betweenness_centrality'] = pd.Series(nx.betweenness_centrality(G, normalized=True, endpoints=False)) 
    df['pagerank'] = pd.Series(nx.pagerank(G, alpha=0.85))
    df['hub'], df['authority'] = nx.hits(G, normalized=True)

    validation = df[df['management'].isnull()]
    validation.drop(['management'], axis=1, inplace=True)
    validation.head()

    training = df[df['management'].notnull()]

    training.head()

    y = training['management']
    x = training.drop(['management'], axis=1)

    from sklearn.ensemble import GradientBoostingClassifier 
    from sklearn.model_selection import GridSearchCV 
    from sklearn.metrics import auc

    gbc = GradientBoostingClassifier()

    parameters = {'n_estimators' : [100, 200, 300], 
                  'max_depth' : [3,5,10],
                  'random_state' : [42] 
                 }

    # parameters = {'n_estimators' : [100], 
    #               'max_depth' : [3],
    #               'random_state' : [42] 
    #              }

    gs = GridSearchCV(gbc, parameters, scoring='roc_auc', cv=10)
    gs.fit(x,y)


    prediction = gs.predict_proba(validation)[:,1]
    prediction = pd.Series(prediction, index=validation.index)
    
    return prediction # Your Answer Here






# ### Part 2B - New Connections Prediction
# 
# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[3]:


future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# In[4]:





# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
# 
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# 
#     Example:
#     
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[29]:


def new_connections_predictions():
    
    # Your Code Here
    #edges metrics: common neighbors, 
    
    #get common neighbors
    n_common_neighbors = [((e[0], e[1]), len(sorted(nx.common_neighbors(G, e[0], e[1])))) for e in nx.non_edges(G)]

    #Jaccard coefficient
    jaccard_coe = [((e[0], e[1]), e[2]) for e in nx.jaccard_coefficient(G)]

    #research allocation
    resource_allocation = [((e[0], e[1]), e[2]) for e in nx.resource_allocation_index(G)]

    #adamic_adar index

    adami_adar = [((e[0], e[1]), e[2]) for e in nx.adamic_adar_index(G)]

    #preferential attachement
    pref_attachement = [((e[0], e[1]), e[2]) for e in nx.preferential_attachment(G)]

    def convert_score_to_series(tupples):
        index = [edge[0] for edge in tupples]
        scores = [edge[1] for edge in tupples]
        scores = pd.Series(scores, index=index)
        return scores

    n_common_neighbors = convert_score_to_series(n_common_neighbors)
    jaccard_coe = convert_score_to_series(jaccard_coe)
    resource_allocation = convert_score_to_series(resource_allocation)
    adami_adar = convert_score_to_series(adami_adar)
    pref_attachement = convert_score_to_series(pref_attachement)

    non_edges_df = pd.concat( [n_common_neighbors, jaccard_coe, resource_allocation, adami_adar, pref_attachement], axis=1)
    non_edges_df.columns = ['n_common_neighbors', 'jaccard_coe', 'resource_allocation', 'adami_adar', 'pref_attachement']
    non_edges_df = non_edges_df.join(future_connections, how='outer')

    validation = non_edges_df[non_edges_df['Future Connection'].isnull()]
    training = non_edges_df[non_edges_df['Future Connection'].notnull()]

    y = training['Future Connection']
    x = training.drop(['Future Connection'], axis=1)
    validation = validation.drop(['Future Connection'], axis=1)

    from sklearn.ensemble import GradientBoostingClassifier 
    from sklearn.model_selection import GridSearchCV 
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import auc

    gbc = GradientBoostingClassifier()
    lr = LogisticRegression()

    # parameters = {'n_estimators' : [100, 200, 300], 
    #               'max_depth' : [3,5,10],
    #               'random_state' : [42] 
    #              }

    parameters = {'penalty' : ['l1', 'l2'], 
                  'C' : [1,2],
                  'random_state' : [42] 
                 }

    gs = GridSearchCV(lr, parameters, scoring='roc_auc', cv=10)
    gs.fit(x,y)

    prediction = gs.predict_proba(validation)[:,1]
    prediction = pd.Series(prediction, index=validation.index)
    return prediction # Your Answer Here



# In[ ]:




